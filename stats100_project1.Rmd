---
title: "Stats 100: Homework 3"
author: "Niall Kehoe"
date: "05/11/2023"
header-includes:
   - \usepackage{bbm, amsmath,amsfonts,amsthm,amssymb,mathrsfs,amsxtra,amscd,latexsym, xcolor, graphicx, fvextra}
   - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
output: pdf_document
---

\newcommand{\Var}{\mathrm{Var}}

```{r}
#in order to run this code, you will need to install these packages. You can do so by going to the install button, or by going to the console and typing, for example, install.packages("knitr")
suppressPackageStartupMessages(library(knitr)) #makes pdfs
suppressPackageStartupMessages(library(latex2exp))
suppressPackageStartupMessages(library(ggplot2)) #makes nice plots
suppressPackageStartupMessages(library(tidyverse))
#good library for data manipulation, includes dplyr and ggplot
# you can read more about the tidyverse at: https://r4ds.had.co.nz/
# you'll need this library for regularization if you use it

#install.packages('glmnet', dependencies=TRUE, type="binary")
#install.packages('predtools', dependencies=TRUE, type="binary")
#install.packages('psych', dependencies=TRUE, type="binary")

suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(predtools))
suppressPackageStartupMessages(library(psych))
knitr::opts_chunk$set(echo = TRUE)
```

# Acknowledgements

NO ACKNOWLEDGEMENTS

# Bradley-Terry Ratings

```{r}

#df_final = read.csv("NBA_2022_23_3pdata_final.csv")
bBall = read.csv("basketball-2022.csv")# %>% filter(Wk < 17)
View(bBall)

# this part is a bit complicated, but my best suggestion is to make a new matrix full of zeroes to fill in

teams = sort(unique(bBall$Visitor.Neutral))
num_teams = length(teams)

# we create our data matrix, with columns for each team, the home field intercept and the pt difference
bt_mat = matrix(0, nrow(bBall), num_teams + 2)
# fill in the list of column names for your data matrix
colnames(bt_mat) = c('HFA', 'Pt_Diff', teams)

# then write a for loop to, for each game in your original data, fill in the point diff, home field, and each team's column appropriately

# for each game, add the home and away team
for (i in 1:nrow(bBall)) {
#for (i in 1:5) {
  # we want 1 for the home team, -1 for the away, so we'll use the Away column which tells us if the winner was Away or not
  bt_mat[i,bBall$Home.Neutral[i]] = 1
  bt_mat[i,bBall$Visitor.Neutral[i]]  = -1
  
}

# we define 1 for home and -1 for away so the HFA is always the same
bt_mat[,'HFA'] = 1

# we should multiply the point diff by -1 if the winning team was away
bt_mat[,'Pt_Diff'] = abs(bBall$PTS - bBall$PTS.1) # equivelant??? 

# if you want to do the unregularized models, create a dataframe

head(bt_mat)
```

## 2.2

Out of four possible versions of Bradley-Terry, some combination of Normal vs Binomial and no regularization vs regularization, fit two of these in R to your data. If your data is only outcomes, then fit the two versions of Binomial Bradley-Terry. If your data includes point differential then you could either use Normal or Binomial Bradley-Terry. \textit{Hint:} To fit the standard model with lm() or glm(), you need the data in a dataframe. But to fit regularization you need the data as a matrix. [\textbf{8 points}]

```{r}

# TIPS FOR NORMAL/BINOMIAL BRADLEY-TERRY
# in both, make sure to exclude the intercept term, and pick the correct response (point diff vs game outcome)

# for NORMAL, use the lm function with your dataframe dat
# MAKE SURE TO EXCLUDE ONE TEAM'S COLUMN, TO SET AS A 0 BETA VALUE

bt_df = data.frame(bt_mat)
#names(bt_df)

# 1) Normal, No regularisation
bt_normal = lm(Pt_Diff ~ . - Washington.Wizards - 1, data = bt_df)
summary(bt_normal)

# for BINOMIAL, use the glm function, with the family = binomial() argument
# otherwise the syntax is the same. MAKE SURE TO STILL EXCLUDE ONE COLUMN

# 2) Binomial, No regularisation
bt_bin = glm(Pt_Diff > 0 ~ . - Washington.Wizards - 1, data = bt_df, family = binomial())
summary(bt_bin)
```

Applying regularization is more challenging, but I'd encourage you to try it out if you'd like. The problem is designed so it is optional, you can always just do the unregularized Normal and Binomial models. See the R file on Canvas for an example of implementing regularization.

```{r}

# TIPS FOR BRADLEY-TERRY WITH REGULARIZATION

# regularization is done with the glmnet library and function
# you need to make a matrix X and response vector y
# then you can use cv.glmnet(), with a vector of lambdas
# use argument alpha = 0, which indicates ridge regularization


# after fitting the cv.glmnet(), you can use the 'best' lambda with the predict function, with the name of your fit, the X vector, and argument s = 'lambda_min'

```

## 2.3

Compare the results of each model in at least two different ways. Some examples include comparing the predictions on your training dataset, comparing the ranking of teams and relative sizes of their coefficients, comparing accuracy of predicting test data (such as the bowl games in the NCAA football data), etc. [\textbf{4 points}]

```{r}

# I won't give scaffolding for this, but here are some tips
# but consider using model$coefficients on your model object to see coefficients
# the sort function might be helpful, with argument decreasing = TRUE
# First comparison, examining which teams agrees and which are disputed
names(sort(bt_normal$coefficients[-1], decreasing=TRUE))
print("-----")
sort(bt_bin$coefficients[-1], decreasing=TRUE)

rankingNormal   = sort(bt_normal$coefficients[-1], decreasing=TRUE)[1:5] # -1 to remove HFA
rankingBinomial = sort(bt_bin$coefficients[-1], decreasing=TRUE)[1:5]    # -1 to remove HFA
#print(rankingNormal)
rankingNormalNames   = names(rankingNormal)
rankingBinomialNames = names(rankingBinomial)

teamsAgreed = list()
teamsNotAgreed = list()
for (item in rankingNormalNames) {
  if (item %in% rankingBinomialNames) {
    #print(item)
    #teamsAgreed.append(item)
    teamsAgreed <- c(teamsAgreed, item)
  } else {
    teamsNotAgreed <- c(teamsNotAgreed, item)
  }
}

print("Teams agreed upon in top 5: ")
for (item in teamsAgreed) {
  print(item)
}
print("--------")
print("Teams disagreed upon in top 5: ")
for (item in teamsNotAgreed) {
  print(item)
}
```

```{r}

# you can predict with the predict() function, and with those predictions determine which team the model thinks will win a game
bowlGames = read.csv("power5fbs.csv") %>% filter(Wk >= 17)
#View(bowlGames)

# we create our data matrix, with columns for each team, the home field intercept and the pt difference
bt_bowl_mat = matrix(0, nrow(bowlGames), num_teams + 2)
# fill in the list of column names for your data matrix
colnames(bt_bowl_mat) = c('HFA', 'Pt_Diff', teams)

# then write a for loop to, for each game in your original data, fill in the point diff, home field, and each team's column appropriately
# for each game, add the home and away team
for (i in 1:nrow(bowlGames)) {
  # we want 1 for the home team, -1 for the away, so we'll use the Away column which tells us if the winner was Away or not
  bt_bowl_mat[i,bowlGames$Winner[i]] = 2 * (bowlGames$Site[i] != '@') - 1
  bt_bowl_mat[i,bowlGames$Loser[i]]  = 2 * (bowlGames$Site[i] == '@') - 1
}

# we define 1 for home and -1 for away so the HFA is always the same
bt_bowl_mat[,'HFA'] = 1
# we should multiply the point diff by -1 if the winning team was away
bt_bowl_mat[,'Pt_Diff'] = (bowlGames$Pts - bowlGames$Pts.1) * (2 * (bowlGames$Site != '@') - 1)

# in order to predict on test data, you will have to create either a new dataframe or matrix with the same number of columns/labeling format

bt_bowl_df = data.frame(bt_bowl_mat)
bt_normal_pred   = predict(bt_normal, bt_bowl_df)
bt_binomial_pred = predict(bt_bin, bt_bowl_df)

# For this bowl games all were won by the first team so this will work
bt_normal_pred_accuracy   = bt_normal_pred > 0
bt_binomial_pred_accuracy = bt_binomial_pred > 0

bt_normal_correct   = sum(bt_normal_pred_accuracy)
bt_binomial_correct = sum(bt_binomial_pred_accuracy)

print(bt_normal_correct / nrow(bt_bowl_df))
print(bt_binomial_correct / nrow(bt_bowl_df))

```

The models agrees that both Georgia and Tennessee are in the top 5 teams but disagree on Kansas State, Texas Christian, and Texas.

The normal model outperformed the normal model on predicting the bowl results in the games (80.95238% vs 52.38095%)

## 2.4

In either model you fit, what are some model assumptions that you find problematic that might lead to shortcomings in the results? [\textbf{2 points}]

Shortcomings: styles of play affecting which teams beat which, injuries affecting performance, small number of games in training set, trading of players mid season.

# Problem 3: Elo Ratings

In this problem you will practice the task of updating Elo ratins based on game results, in the process analyzing the properties of the model. In this problem we will reference an existing {\color{blue}\href{http://tennisabstract.com/reports/wta_elo_ratings.html}{Elo system}} for the WTA tour. If you'd like you can look at {\color{blue}\href{https://github.com/sleepomeno/tennis_atp/blob/master/examples/elo.R}{the code}} actually used for the Elo algorithm, which is the same as our model but has a different value of $K$ based on each player's number of games played.

## 3.1

As of the time of writing, Iga Swiatek, Aryna Sabalenka, and Coco Gauff are ranked 1, 3, and 9 respectively in the Elo, with respective ratings of 2199, 2088, and 1982. For tennis we'll assume that the update algorithm uses update function $$\Upsilon(R_i-R_j)=K\cdot\frac{1}{1+e^{(R_i-R_j)/{s}}},$$ where $K=40$ and $s=174$. Suppose the three play a round robin, with (in order) Swiatek beating Sabalenka, Sabalenka beating Gauff, and Gauff beating Swiatek. What will the new Elo ratings of each player be after this round robin? Feel free to do all this manually or write a function in R to update ratings. Details on the Elo model [\textbf{4 points}]

$$\Upsilon(R_i-R_j)=\frac{40}{1+e^{(R_i-R_j)/{174}}},$$

Swiatek = 2199, Sabalenka = 2088, Gauff = 1982

Here are the general steps involved in updating Elo rankings:

1.  Determine the starting rating: Each player or team starts with a certain Elo rating, which is typically based on their prior performance in the game or competition.

2.  Calculate the expected score: Based on the Elo ratings of the two players or teams, a formula is used to estimate the probability of each player or team winning the game. The expected score is the predicted score for each player or team.

3.  Calculate the actual score: After the game is played, the actual score is determined. For example, in chess, a win is worth one point, a draw is worth half a point, and a loss is worth zero points.

4.  Calculate the rating change: The difference between the actual score and the expected score is used to calculate the rating change for each player or team. The formula for calculating the rating change depends on the specific Elo system being used.

5.  Update the ratings: The new ratings for each player or team are calculated by adding the rating change to their previous rating.

6.  Repeat for each game: The process is repeated for each game played, with the new ratings serving as the starting ratings for the next game.

    $$R_i - R_j = 2199 - 2088 = 111$$

    $$\Upsilon(R_i-R_j)=\frac{40}{1+e^{\frac{111}{174}}} = 13.8285752602$$

```{r}

elos <<- c(2199, 2088, 1982)
print(elos)
e = 2.71828

update_elo <- function(a,b) { # winner is first, loser second
  rA = elos[a]
  rB = elos[b]

  eA <- 40 / (1 + e^((rB - rA)/174))
  eB <- 40 / (1 + e^((rA - rB)/174))
  
  # Winner is player A and gets one point
  #sA <- 1
  #sB <- 0
  
  #rA_new <- rA + (k*kA) * (sA-eA)
  #rB_new <- rB + (k*kB) * (sB-eB)
  
  rA_new = rA + eA
  rB_new = rB - eB

  
  elos[a] <<- rA_new
  elos[b] <<- rB_new
  
  print(elos)
}

#Swiatek = 2199, Sabalenka = 2088, Gauff = 1982
#Swiatek beating Sabalenka, Sabalenka beating Gauff, and Gauff beating Swiatek

update_elo(1,2) # Swiatek beating Sabalenka = 1 beating 2
update_elo(2,3) # Sabalenka beating Gauff   = 2 beating 3
update_elo(3,1) # Gauff beating Swiatek     = 3 beating 1

print("----------")
print(elos)
```

Elo Rankings: Swiatek = 2192.572, Sabalenka = 2099.348, Gauff = 1974.577.

## 3.2

Repeat the computation, but assuming that the round robin occurred in the opposite order, so Gauff beats Swiatek, then Sabalenka beats Gauff, then Sabalenka beats Swiatek. What are the new Elo ratings in this case? Are they different from part 1? Why does this make sense given the structure of the Elo model?

[\textbf{4 points}]

```{r}

elos <<- c(2199, 2088, 1982)
print(elos)

#Swiatek = 2199, Sabalenka = 2088, Gauff = 1982
# Gauff beats Swiatek, then Sabalenka beats Gauff, then Swiatek beats Sabalenka

update_elo(3,1) # Gauff beating Swiatek     = 3 beating 1
update_elo(2,3) # Sabalenka beating Gauff   = 2 beating 3
update_elo(1,2) # Swiatek beating Sabalenka = 1 beating 2

print("----------")
print(elos)
```

Elo Rankings: Swiatek = 2191.034, Sabalenka = 2096.545, Gauff = 1976.366.

The rankings are different than in part 1. This makes sense since the elo updates are based off the values of the players involved elo at the time the match took place. By changing the order of the matches, the elo of the players for the 2nd and 3rd games have also changed due to being updated differently after the first game.

## 3.3

These Elo ratings are computed in a different way from the {\color{blue}\href{https://www.wtatennis.com/rankings/singles}{official WTA ratings}}, which assign points based on the round of a tournament in which a win occurred, scaling with the level of a tournament. How does this compare with the Elo system, and which do you think would perform better in prediction? [\textbf{2 points}]

I think that the Elo model would outperforms the tournament tier model as the level of the tournament is a subjective matter and can contribute to inaccuracies. The Elo model is less subjective and less volatile. As one disappointing tournament can lead to a large drop in ranking under the tournament system, this seems unfair and inaccurate. Furthermore, rising players will take longer to be incorperated into the top ranks as they must compete in many tournaments in the top tier before they can be accurately mapped.

# Problem 4 (not graded)

How many hours did you spend on this HW?

8 hours
